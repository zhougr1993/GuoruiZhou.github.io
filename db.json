{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/jacman/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/FontAwesome.otf","path":"font/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.woff","path":"font/coveredbyyourgrace-webfont.woff","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.ttf","path":"font/coveredbyyourgrace-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.eot","path":"font/coveredbyyourgrace-webfont.eot","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/fontawesome-webfont.eot","path":"font/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/fontdiao.eot","path":"font/fontdiao.eot","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/fontdiao.ttf","path":"font/fontdiao.ttf","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/fontdiao.woff","path":"font/fontdiao.woff","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/fontawesome-webfont.woff","path":"font/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/cc-by-nc-nd.svg","path":"img/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/cc-by-nc-sa.svg","path":"img/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/cc-by-nc.svg","path":"img/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/cc-by-nd.svg","path":"img/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/cc-by-sa.svg","path":"img/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/cc-by.svg","path":"img/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/cc-zero.svg","path":"img/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/favicon.ico","path":"img/favicon.ico","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/jacman.jpg","path":"img/jacman.jpg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/logo.svg","path":"img/logo.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/scrollup.png","path":"img/scrollup.png","modified":0,"renderable":1},{"_id":"themes/jacman/source/js/gallery.js","path":"js/gallery.js","modified":0,"renderable":1},{"_id":"themes/jacman/source/js/jquery.qrcode-0.12.0.min.js","path":"js/jquery.qrcode-0.12.0.min.js","modified":0,"renderable":1},{"_id":"themes/jacman/source/js/totop.js","path":"js/totop.js","modified":0,"renderable":1},{"_id":"themes/jacman/source/js/jquery.imagesloaded.min.js","path":"js/jquery.imagesloaded.min.js","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.svg","path":"font/coveredbyyourgrace-webfont.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/fontawesome-webfont.ttf","path":"font/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/fontdiao.svg","path":"font/fontdiao.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/author.jpg","path":"img/author.jpg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/logo.png","path":"img/logo.png","modified":0,"renderable":1},{"_id":"themes/jacman/source/js/jquery-2.0.3.min.js","path":"js/jquery-2.0.3.min.js","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/jacman/source/font/fontawesome-webfont.svg","path":"font/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/jacman/source/img/banner.jpg","path":"img/banner.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/jacman/.gitignore","hash":"7d65523f2a5afb69d76824dd1dfa62a34faa3197","modified":1475649192000},{"_id":"themes/jacman/LICENSE","hash":"931516aa36c53eb7843c83d82662eb50cc3c4367","modified":1475649192000},{"_id":"themes/jacman/README.md","hash":"75a5c9fbd7c9cec4d2f277042d2fee550e4936be","modified":1475649192000},{"_id":"themes/jacman/README_zh.md","hash":"d6014b16eaccc97dc54a7779c9e36003752410e1","modified":1475649192000},{"_id":"themes/jacman/_config.yml","hash":"011a237dafa143327a8ed48a4fee9a13782bec78","modified":1475651766000},{"_id":"source/_posts/RL-for-seq2seq.md","hash":"511509960b07f3eedb609e835489decdbe021763","modified":1475743970000},{"_id":"themes/jacman/languages/default.yml","hash":"eea72d6138497287c0b3f4bd93e4f6f62b7aff37","modified":1475649192000},{"_id":"themes/jacman/languages/zh-CN.yml","hash":"1f3b9d00dd4322352b0c9c82a76dc9865a616d41","modified":1475649192000},{"_id":"themes/jacman/languages/zh-TW.yml","hash":"61a02ba818d641579a86fcd7f5926ab1e6ab5f70","modified":1475649192000},{"_id":"themes/jacman/layout/archive.ejs","hash":"a18842e3d719fe3ca9b977a6995f8facc75c8673","modified":1475649192000},{"_id":"themes/jacman/layout/category.ejs","hash":"9b740fc33f6f028df60f0bc4312bf3ebd03aa8ea","modified":1475649192000},{"_id":"themes/jacman/layout/index.ejs","hash":"75cef2172c286994af412e11ab7f4f5a0daaf1f5","modified":1475649192000},{"_id":"themes/jacman/layout/layout.ejs","hash":"5b4289a4526899809b9c2facea535367ff51ba2b","modified":1475649192000},{"_id":"themes/jacman/layout/page.ejs","hash":"bd6bbf2ea8e183bd835867ff617dc6366b56748c","modified":1475649192000},{"_id":"themes/jacman/layout/post.ejs","hash":"3114134775bdde5a83cf14feb019606fa2b2b2be","modified":1475649192000},{"_id":"themes/jacman/layout/tag.ejs","hash":"45150a2365768b6b67880193c9264ad2bb4814db","modified":1475649192000},{"_id":"themes/jacman/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/after_footer.ejs","hash":"c703b0c25139b8a5f8f9d24a334a07905e2b7987","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/analytics.ejs","hash":"697601996220fe0a0f9cd628be67dec3c86ae2aa","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/archive.ejs","hash":"2c7395e7563fe016521712a645c28a13f952d52a","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/article.ejs","hash":"261ecacb8456f4cb972632b6a9103860fa63b9a3","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/article_row.ejs","hash":"4cb855d91ece7f67b2ca0992fffa55472d0b9c93","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/categories.ejs","hash":"8a52d0344d5bce1925cf586ed73c11192925209b","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/footer.ejs","hash":"5f80bf6c6ddcf8c28c4599cd1540b14b25d54f18","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/head.ejs","hash":"761941be4922cd3c177c8130296b909bf7db5c09","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/header.ejs","hash":"18515612344ff048b9372b91b7eef6f3b143801f","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/mathjax.ejs","hash":"d42994ac696f52ba99c1cbac382cd76d5b04a3e8","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/pagination.ejs","hash":"6146ac37dfb4f8613090bc52b3fc8cfa911a186a","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/search.ejs","hash":"1083824a6c6c3df02767f2f3b727aee78ebb76ec","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/sidebar.ejs","hash":"c4f527fff0070fbe65919053a16224412317f40d","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/tags.ejs","hash":"b33b2b5d08f1d53a8de25a95f660f7f1cea7b3cb","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/tinysou_search.ejs","hash":"06ecddc8a9d40b480fe2e958af1dab857a9d5441","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/totop.ejs","hash":"bea5bb7cb9350b8af7d97a8d223af63a5b30ab78","modified":1475649192000},{"_id":"themes/jacman/layout/_widget/archive.ejs","hash":"39ea6b7888406fbd1b4cf236ebd718e881493374","modified":1475649192000},{"_id":"themes/jacman/layout/_widget/category.ejs","hash":"c1fae96b5053da021bcc04ab2ce5c2c8d30de8a2","modified":1475649192000},{"_id":"themes/jacman/layout/_widget/douban.ejs","hash":"e3820c36169e88663e6c9177666b2904c1ce47e6","modified":1475649192000},{"_id":"themes/jacman/layout/_widget/github-card.ejs","hash":"5c759b6ea214bac56a393247de27e67ce73fb33f","modified":1475649192000},{"_id":"themes/jacman/layout/_widget/links.ejs","hash":"e49868063439c2092cdf9a8ec82cc295b0e42f66","modified":1475649192000},{"_id":"themes/jacman/layout/_widget/rss.ejs","hash":"0a4b5f2a2e36a1d504fe2e7c6c8372cbb4628aab","modified":1475649192000},{"_id":"themes/jacman/layout/_widget/tag.ejs","hash":"7e82ad9c916b9ce871b2f65ce8f283c5ba47947b","modified":1475649192000},{"_id":"themes/jacman/layout/_widget/tagcloud.ejs","hash":"10a1001189d5c28ce6d42494563b9637c302b454","modified":1475649192000},{"_id":"themes/jacman/layout/_widget/weibo.ejs","hash":"a31c2b223d0feb2a227e203cac9e5d13b7d328a8","modified":1475649192000},{"_id":"themes/jacman/source/css/style.styl","hash":"a0a45af186a72ae68979bf26f2a5d0d2303189ca","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1475649192000},{"_id":"themes/jacman/source/font/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1475649192000},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.woff","hash":"c6f8dc1a2f6ce914f120e80a876b8fd77b98888e","modified":1475649192000},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.ttf","hash":"194ccb4acf77a03dc25bcc174edb266143704fec","modified":1475649192000},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.eot","hash":"a17d0f10534303e40f210c506ebb8703fa23b7de","modified":1475649192000},{"_id":"themes/jacman/source/font/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1475649192000},{"_id":"themes/jacman/source/font/fontdiao.eot","hash":"9544a0d7ba208989302bc4da5a184faeb0e883c9","modified":1475649192000},{"_id":"themes/jacman/source/font/fontdiao.ttf","hash":"ee9fd7be2493c9bf6d2841044e69a0830d9d3fab","modified":1475649192000},{"_id":"themes/jacman/source/font/fontdiao.woff","hash":"71f54eb6e98aa28cafeb04aab71c0e5b349ea89f","modified":1475649192000},{"_id":"themes/jacman/source/font/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1475649192000},{"_id":"themes/jacman/source/img/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1475649192000},{"_id":"themes/jacman/source/img/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1475649192000},{"_id":"themes/jacman/source/img/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1475649192000},{"_id":"themes/jacman/source/img/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1475649192000},{"_id":"themes/jacman/source/img/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1475649192000},{"_id":"themes/jacman/source/img/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1475649192000},{"_id":"themes/jacman/source/img/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1475649192000},{"_id":"themes/jacman/source/img/favicon.ico","hash":"2d22a3e0c7905a894e832c831dd91c29c209c7a5","modified":1475649192000},{"_id":"themes/jacman/source/img/jacman.jpg","hash":"0ba14a4a5e3be012826fc713c33479912126d34e","modified":1475649192000},{"_id":"themes/jacman/source/img/logo.svg","hash":"9ae38f7225c38624faeb7b74996efa9de7bf065b","modified":1475649192000},{"_id":"themes/jacman/source/img/scrollup.png","hash":"2137d4f1739aa8aa3fcb0348c3ddf1e41d62f2e3","modified":1475649192000},{"_id":"themes/jacman/source/js/gallery.js","hash":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1475649192000},{"_id":"themes/jacman/source/js/jquery.qrcode-0.12.0.min.js","hash":"57c3987166a26415a71292162690e82c21e315ad","modified":1475649192000},{"_id":"themes/jacman/source/js/totop.js","hash":"cad23c5ea7163d1e5c05a0fd3ef9233469da10cb","modified":1475649192000},{"_id":"themes/jacman/source/js/jquery.imagesloaded.min.js","hash":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1475649192000},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.svg","hash":"eabdb262d8e246865dfb56031f01ff6e8d2f9d53","modified":1475649192000},{"_id":"themes/jacman/source/font/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1475649192000},{"_id":"themes/jacman/source/font/fontdiao.svg","hash":"334a94e6a66a8b089be7315d876bec93efe38d2b","modified":1475649192000},{"_id":"themes/jacman/source/img/author.jpg","hash":"91b2a34cb3316ac7354b7f432fd3870a56c58fb5","modified":1475649776000},{"_id":"themes/jacman/source/img/logo.png","hash":"fd08d12d1fa147cf894e8f8327e38f1758de32ed","modified":1475649192000},{"_id":"themes/jacman/source/js/jquery-2.0.3.min.js","hash":"a0ae3697b0ab8c0e8bd3186c80db42abd6d97a8d","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/post/article.ejs","hash":"b09e3acea7076e1f01dfe0c2295e19951ea09437","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/post/comment.ejs","hash":"c88bc8f5805173920a5fdd7e9234a850e3d8e151","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/post/catetags.ejs","hash":"0e37bababc8f4659f5b59a552a946b46d89e4158","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/post/footer.ejs","hash":"b12ec08a5845a3d8c01257614f1dfead879c87d2","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/post/gallery.ejs","hash":"fafc2501d7e65983b0f5c2b58151ca12e57c0574","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/post/header.ejs","hash":"36a705942b691abe0d643ea8afa339981b32f6f2","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/post/jiathis.ejs","hash":"d7f5960039ac74924559ab6ba03c64457b8f0966","modified":1475649192000},{"_id":"themes/jacman/layout/_partial/post/pagination.ejs","hash":"7de9c07a4c968429a8088c31a28b7f3a993ded1b","modified":1475649192000},{"_id":"themes/jacman/source/css/_base/font.styl","hash":"c8a0faf43b08e37ad07a5669db76d595da966159","modified":1475649192000},{"_id":"themes/jacman/source/css/_base/public.styl","hash":"f016180726019927b9a835ed01e04d153f27a149","modified":1475649192000},{"_id":"themes/jacman/source/css/_base/variable.styl","hash":"cb652eb83c28a208743fabab92de896f8b7cbf7b","modified":1475649192000},{"_id":"themes/jacman/source/css/_partial/article.styl","hash":"c69641b4a34a8c62986b335414413dbde26de25e","modified":1475649192000},{"_id":"themes/jacman/source/css/_partial/aside.styl","hash":"506fde1d67ce750452cbe84bee01a19c7d027c5e","modified":1475649192000},{"_id":"themes/jacman/source/css/_partial/footer.styl","hash":"1911613a19b605a58f801c21b03b5d4c83b90f9c","modified":1475649192000},{"_id":"themes/jacman/source/css/_partial/duoshuo.styl","hash":"e85f1192283f043115c272a9deb3cb6ced793990","modified":1475649192000},{"_id":"themes/jacman/source/css/_partial/gallery.styl","hash":"7246809f4ce3166ec1b259bf475cae1a48e29aad","modified":1475649192000},{"_id":"themes/jacman/source/css/_partial/header.styl","hash":"5121ceb712be3f2dde98b8b6e589b546e19eab8f","modified":1475649192000},{"_id":"themes/jacman/source/css/_partial/helper.styl","hash":"1136600932b97534b88465bf05ef313630b2de3d","modified":1475649192000},{"_id":"themes/jacman/source/css/_partial/index.styl","hash":"a72ff14effd276015264f870f47ed8f8413bf5d3","modified":1475649192000},{"_id":"themes/jacman/source/css/_partial/totop.styl","hash":"96363d7c5aaed5f649667fc0752a62620a67e872","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1475649192000},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1475649192000},{"_id":"themes/jacman/source/font/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1475649192000},{"_id":"themes/jacman/source/img/banner.jpg","hash":"5104860c4f8b2e84ef734ba6c37fe7a288bf0d74","modified":1475649192000},{"_id":"themes/jacman/source/css/_base/highlight/highlight.styl","hash":"91b62bfc58390b0d5db782a75be6965ee3665eb3","modified":1475649192000},{"_id":"themes/jacman/source/css/_base/highlight/theme.styl","hash":"e3a59bd427ba37a54ead9eeba9a5356b3f720a48","modified":1475649192000},{"_id":"public/atom.xml","hash":"49078c2683e6578976f1b446053234cc66238146","modified":1475744121878},{"_id":"public/archives/index.html","hash":"d9ca55f2fbfbf011f18f46f2d7b25b6850fbea7b","modified":1475744121938},{"_id":"public/archives/2016/index.html","hash":"6847b8c81a485520d28a2770bd9f38966b9b7d06","modified":1475744121938},{"_id":"public/archives/2016/10/index.html","hash":"558486166ce47c4b17807b90d8e07e3060b7ed2c","modified":1475744121939},{"_id":"public/2016/10/05/RL-for-seq2seq/index.html","hash":"2756cad8204839b0eb755be483f8dc1b83adf968","modified":1475744121939},{"_id":"public/index.html","hash":"b59442d58fd975f6ca8f007b7f2de8780cea69fb","modified":1475744121939},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1475744121946},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1475744121946},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1475744121946},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1475744121947},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1475744121948},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1475744121948},{"_id":"public/font/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1475744121948},{"_id":"public/font/coveredbyyourgrace-webfont.woff","hash":"c6f8dc1a2f6ce914f120e80a876b8fd77b98888e","modified":1475744121948},{"_id":"public/font/coveredbyyourgrace-webfont.ttf","hash":"194ccb4acf77a03dc25bcc174edb266143704fec","modified":1475744121949},{"_id":"public/font/coveredbyyourgrace-webfont.eot","hash":"a17d0f10534303e40f210c506ebb8703fa23b7de","modified":1475744121949},{"_id":"public/font/fontdiao.eot","hash":"9544a0d7ba208989302bc4da5a184faeb0e883c9","modified":1475744121949},{"_id":"public/font/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1475744121949},{"_id":"public/font/fontdiao.ttf","hash":"ee9fd7be2493c9bf6d2841044e69a0830d9d3fab","modified":1475744121949},{"_id":"public/font/fontdiao.woff","hash":"71f54eb6e98aa28cafeb04aab71c0e5b349ea89f","modified":1475744121949},{"_id":"public/img/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1475744121950},{"_id":"public/img/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1475744121950},{"_id":"public/img/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1475744121950},{"_id":"public/img/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1475744121950},{"_id":"public/img/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1475744121950},{"_id":"public/img/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1475744121950},{"_id":"public/img/favicon.ico","hash":"2d22a3e0c7905a894e832c831dd91c29c209c7a5","modified":1475744121950},{"_id":"public/img/jacman.jpg","hash":"0ba14a4a5e3be012826fc713c33479912126d34e","modified":1475744121950},{"_id":"public/img/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1475744121951},{"_id":"public/img/logo.svg","hash":"9ae38f7225c38624faeb7b74996efa9de7bf065b","modified":1475744121951},{"_id":"public/font/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1475744121951},{"_id":"public/img/scrollup.png","hash":"2137d4f1739aa8aa3fcb0348c3ddf1e41d62f2e3","modified":1475744121951},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1475744121952},{"_id":"public/font/fontdiao.svg","hash":"334a94e6a66a8b089be7315d876bec93efe38d2b","modified":1475744122761},{"_id":"public/font/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1475744122764},{"_id":"public/font/coveredbyyourgrace-webfont.svg","hash":"eabdb262d8e246865dfb56031f01ff6e8d2f9d53","modified":1475744122765},{"_id":"public/img/author.jpg","hash":"91b2a34cb3316ac7354b7f432fd3870a56c58fb5","modified":1475744122765},{"_id":"public/img/logo.png","hash":"fd08d12d1fa147cf894e8f8327e38f1758de32ed","modified":1475744122765},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1475744122768},{"_id":"public/js/gallery.js","hash":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1475744122768},{"_id":"public/js/totop.js","hash":"cad23c5ea7163d1e5c05a0fd3ef9233469da10cb","modified":1475744122768},{"_id":"public/js/jquery.imagesloaded.min.js","hash":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1475744122768},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1475744122768},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1475744122769},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1475744122769},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1475744122769},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1475744122769},{"_id":"public/css/style.css","hash":"1737965b2ad2e4999c93bfa7657bdfc9990675f3","modified":1475744122769},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1475744122769},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1475744122769},{"_id":"public/js/jquery.qrcode-0.12.0.min.js","hash":"57c3987166a26415a71292162690e82c21e315ad","modified":1475744122769},{"_id":"public/js/jquery-2.0.3.min.js","hash":"a0ae3697b0ab8c0e8bd3186c80db42abd6d97a8d","modified":1475744122769},{"_id":"public/font/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1475744122773},{"_id":"public/img/banner.jpg","hash":"5104860c4f8b2e84ef734ba6c37fe7a288bf0d74","modified":1475744122774}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Notes on Sequence Level Training with Recurrent Neural Networks","date":"2016-10-05T07:20:09.000Z","_content":"# Sequence级别的seq2seq训练过程，用强化学习来优化整个句子效果\n\n[原论文连接](http://arxiv.org/abs/1511.06732)\n\n###Summary\n\n最近突然想把自己看过的东西记下来，决定开通博客啦，就以这篇seq2seq领域的论文开始吧。\n\n先简单介绍一下seq2seq是什么。\nprocessing of seq2seq：\n![](https://www.tensorflow.org/versions/r0.11/images/basic_seq2seq.png)\n借用一下tensorflow里Sequence-to-Sequence的图。这个任务的目标简单的说就是将一个序列翻译成另一序列。\n\n具体到这张图里我们的输入是有序的A,B,C序列，目标序列是输出X,Y,Z。其中每一个白色的方框是RNN的一个Cell，输入部分和输出部分的RNN分别共享两套参数。整个过程可看做将输入的A,B,C通过输入RNN编码为一个Cell code，输出部分根据上一时态的输入以及Cell状态预估下一个输出词的概率分布即<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script> \\\\(P(w_{t+1}^g|h_{t+1},w_t)\\\\) 根据此概率分布找出最适合的输出，可抽象为一个encode之后decode的过程。\n\n此类过程可以做很多工作，比如机器翻译讲英文翻译为中文，摘要抽取将长文转换为单字信息量更大的短文，或者是人机对话的QA问答都可以抽象成类似的过程。\n\n### Motivation\n回到这篇论文来看，它主要目标是试图解决seq2seq领域的两个重要问题：\n\n- Exposure Bias: seq2seq的Training阶段的decode过程，我们是有ground truth的，即训练decode过程的RNN时我们可以输入真实的序列上一个time step的词，这种训练过程叫XENT。但是在Testing阶段的decode过程我们并不知道上一个time step的词是什么，我们输入给model的是一个预测出来概率最大的词，如果这个词错了，误差会随后续过程传递并且模型没有机会纠正。\n\n- Loss-Evaluation Mismatch: 我们训练模型的时候，采用的loss是word级别的交叉熵，但是我们最终评判模型的指标是sequence级别的指标，比如机器翻译的BLEU.\n\n### Related Work\n对应于Exposure Bias，该论文之前有两种解决方法：\n\n- beam search\n    算法原理：图路径搜索中，每一步深度扩展的时候，剪掉一些质量比较差的结点，    保留下一些质量较高的结点，这样就减少了空间消耗，并提高了时间效率，但缺点就是    有可能存在潜在的最佳方案被丢弃，其中保留的质量较高路径的个数称为beam size.（走n步吃到嘴多的果实）\n    ![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475738457157.png\n)\n应用到seq2seq里就是每一次解码过程看成是一步深度扩展，每一次解码预测概率最大的beam size个词就是候选结点，并累积概率选择概率和最高的beam size条路径，把整个解码过程看成是寻求最大联合概率的图搜索过程。该方法能解决Exposure bias的原因在于它使解码过程不仅仅依赖于前一个词输出，还要满足全局解码概率最大，因而原始模型前一个预测错误而带来的误差传递的可能性就降低了。\n\n- DaD (Data as Demonstrator)\n对于Exposure bias造成解码结果不好的原因，bengio解释为由于训练和预测过程在输入数据的分布不同，前者是样本的数据分布，后者则是decode模型的输出分布。因而解决办法就是保证两个流程在解码的时候输入参数服从相同分布，即都采用前一个词的预测结果作为当前词的输入。\n为此提出了一种退火算法来解决这个问题，在Training过程中引入一个概率值参数，称其为温度，当温度值较大时高概率采用ground truth词\\\\(w_t\\\\)输入，当值较小时，则高概率采用预测输出词\\\\(w_t^g\\\\)作为下一个输入，随着迭代次数的增加，该参数趋近于0.即完全采用前一个词的预测\\\\(w_t^g\\\\)作为输入。\nTraining的decode部分如下图：\\\\(w_t^g\\\\)为模型预测词，\\\\(w_t\\\\)为ground truth 后文中所有的带上标g的都为模型预测输出。\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475739099129.png)\n### Model\n此文借鉴了DaD的训练过程，同时由于有些目标如(BLEU)不能被直接优化，作者在此文中提出了一种新的训练流程，能可观的提高此类目标的效果。\n核心思想是利用强化学习，强化学习可以通过随机的递归生成结果来寻找优化那些复杂的目标。作者将seq2seq的训练过程对应于强化学习抽象成了这么几个部分：\n\n- action: 每一个time step 的候选词\n- state: 每一个time step RNN隐层的神经元状态\n- reward: 整个后续sequence解码完后的bleu等指标\n\n这里需要强调一点，reward是整个sequence解码完的指标，所以作者题目是sequence级别的训练，不是word级别，此文的目标也是优化最后整个生产sequence的效果。\n\n因此作者将Reinforce Loss函数定义为整个生成出的sequence拿到的负reward:\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740434191.png\n)\n对参数\\\\(\\theta\\\\)求导:\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740913152.png\n)\n其中\\\\(O_t\\\\)是softmax的输入。\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740768186.png\n)\n这个公式的推导可以去看另一篇用RL的文章 https://arxiv.org/abs/1505.00521\n需要注意的是 这个导数的右半部分其实就是交叉熵Loss(XENT指在decode部分输入为ground truth的训练方式)的求导结果:\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475741346812.png\n)\n而左半部分是整个被生成序列的reward减去t时刻状态\\\\(S_t\\\\)下所能得到的平均reward，这是强化学习比较常见的一个公式。\n这个推导结果的物理意义可以很直观的解释，即在t+1时刻右端交叉熵Loss的限制下，左端要求我们选择被生成的词最后构成的reward \\\\(r(w_1^g,...,w_T^g)\\\\)要大于在t+1时刻时态的一个可获得的平均reward \\\\({\\bar{r}}_{t+1}\\\\)。\n\n前面提到是sequence级别的训练，所以最终每个时刻的loss要整个sequence全decode后才能得到，而目前我们唯一不知道的量就是reward \\\\({\\bar{r}}_{t+1}\\\\)。在其他场景下，这个值可以用随机的生成结果来采样取得，本文作者用了非常简单的线性回归来预估这个值，输入为当前RNN模型的所有隐层节点状态，Loss为与真值reward二范数距离: \\\\({||\\bar{r}-r||}^2\\\\)。\n\n### 训练过程\n偷个懒直接截原文图再来解释整个过程:\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475742637703.png)\n\n这里面\\\\(N^{XENT}\\\\)和\\\\(N^{XE+R}\\\\)分别是两个可调的超参，分别代表两个训练过程的训练次数，XENT代表传统的交叉熵Loss训练并且decode时每一个时刻的输入为ground truth \\\\(w_t\\\\), XE+R代表的是Loss为前文提到的Reinforce Loss，T代表的是decode部分sequence长度，decode部分每一个时刻的输入为模型预测生成的输出\\\\(w_t^g\\\\)。\n\n整个过程其实就分为两部分，一开始先训练\\\\(N^{XENT}\\\\)个epoch 用传统的XENT训练方式，之后呢在每一个sequence训练的decode部分，前s个step 用XENT交叉熵loss，后T-s个时态用Reinforce Loss训练。然后不断的将s从T开始减少，最后整个sequence都用Reinforce Loss。\n\n\n这里就不贴实验结果图啦，总的来说在BLEU等整句指标上此文的方法都有显著的提高。\n\n### 我的观点\n- 本文的最大贡献其实不在于Exposure Bias部分而在于Loss-Evaluation Mismatch部分，提出了一个有效的训练方法，能优化那些复杂不直观的目标，同时近来reinforce learning在各领域发力，这种类似的训练方法可以值得大家在各个领域尝试一下。\n\n- 训练过程设计得很巧妙，其实作者很聪明也了解reinforce learning的特性，强化学习有一个致命的缺点就是训练过程的不稳定性，作者让训练过程起始时刻用XENT来作为训练，是为了让模型到达一个较好又稳定的状态，之后再用强化学习来优化模型对于整句的目标。如果不这样安排，大可能模型无法收敛到一个好的解。\n\n- 本文对Exposure Bias还是没有有效解决，这一点上我倒觉得还不如DaD，至少DaD在训练过程的后半程退火阶段，Training的decode过程和Testing的decode过程是一致的，而本文中XE+R部分虽然也是用模型生成的词\\\\(w_t^g\\\\)做输入但是Loss是RL Loss并不是完全依赖于交叉熵。这里是不对等的，所以预测过程用beam search依然会极大的提高本文提出的模型的效果。\n","source":"_posts/RL-for-seq2seq.md","raw":"---\ntitle: Notes on Sequence Level Training with Recurrent Neural Networks\ndate: 2016-10-05 15:20:09\ntags:\n---\n# Sequence级别的seq2seq训练过程，用强化学习来优化整个句子效果\n\n[原论文连接](http://arxiv.org/abs/1511.06732)\n\n###Summary\n\n最近突然想把自己看过的东西记下来，决定开通博客啦，就以这篇seq2seq领域的论文开始吧。\n\n先简单介绍一下seq2seq是什么。\nprocessing of seq2seq：\n![](https://www.tensorflow.org/versions/r0.11/images/basic_seq2seq.png)\n借用一下tensorflow里Sequence-to-Sequence的图。这个任务的目标简单的说就是将一个序列翻译成另一序列。\n\n具体到这张图里我们的输入是有序的A,B,C序列，目标序列是输出X,Y,Z。其中每一个白色的方框是RNN的一个Cell，输入部分和输出部分的RNN分别共享两套参数。整个过程可看做将输入的A,B,C通过输入RNN编码为一个Cell code，输出部分根据上一时态的输入以及Cell状态预估下一个输出词的概率分布即<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script> \\\\(P(w_{t+1}^g|h_{t+1},w_t)\\\\) 根据此概率分布找出最适合的输出，可抽象为一个encode之后decode的过程。\n\n此类过程可以做很多工作，比如机器翻译讲英文翻译为中文，摘要抽取将长文转换为单字信息量更大的短文，或者是人机对话的QA问答都可以抽象成类似的过程。\n\n### Motivation\n回到这篇论文来看，它主要目标是试图解决seq2seq领域的两个重要问题：\n\n- Exposure Bias: seq2seq的Training阶段的decode过程，我们是有ground truth的，即训练decode过程的RNN时我们可以输入真实的序列上一个time step的词，这种训练过程叫XENT。但是在Testing阶段的decode过程我们并不知道上一个time step的词是什么，我们输入给model的是一个预测出来概率最大的词，如果这个词错了，误差会随后续过程传递并且模型没有机会纠正。\n\n- Loss-Evaluation Mismatch: 我们训练模型的时候，采用的loss是word级别的交叉熵，但是我们最终评判模型的指标是sequence级别的指标，比如机器翻译的BLEU.\n\n### Related Work\n对应于Exposure Bias，该论文之前有两种解决方法：\n\n- beam search\n    算法原理：图路径搜索中，每一步深度扩展的时候，剪掉一些质量比较差的结点，    保留下一些质量较高的结点，这样就减少了空间消耗，并提高了时间效率，但缺点就是    有可能存在潜在的最佳方案被丢弃，其中保留的质量较高路径的个数称为beam size.（走n步吃到嘴多的果实）\n    ![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475738457157.png\n)\n应用到seq2seq里就是每一次解码过程看成是一步深度扩展，每一次解码预测概率最大的beam size个词就是候选结点，并累积概率选择概率和最高的beam size条路径，把整个解码过程看成是寻求最大联合概率的图搜索过程。该方法能解决Exposure bias的原因在于它使解码过程不仅仅依赖于前一个词输出，还要满足全局解码概率最大，因而原始模型前一个预测错误而带来的误差传递的可能性就降低了。\n\n- DaD (Data as Demonstrator)\n对于Exposure bias造成解码结果不好的原因，bengio解释为由于训练和预测过程在输入数据的分布不同，前者是样本的数据分布，后者则是decode模型的输出分布。因而解决办法就是保证两个流程在解码的时候输入参数服从相同分布，即都采用前一个词的预测结果作为当前词的输入。\n为此提出了一种退火算法来解决这个问题，在Training过程中引入一个概率值参数，称其为温度，当温度值较大时高概率采用ground truth词\\\\(w_t\\\\)输入，当值较小时，则高概率采用预测输出词\\\\(w_t^g\\\\)作为下一个输入，随着迭代次数的增加，该参数趋近于0.即完全采用前一个词的预测\\\\(w_t^g\\\\)作为输入。\nTraining的decode部分如下图：\\\\(w_t^g\\\\)为模型预测词，\\\\(w_t\\\\)为ground truth 后文中所有的带上标g的都为模型预测输出。\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475739099129.png)\n### Model\n此文借鉴了DaD的训练过程，同时由于有些目标如(BLEU)不能被直接优化，作者在此文中提出了一种新的训练流程，能可观的提高此类目标的效果。\n核心思想是利用强化学习，强化学习可以通过随机的递归生成结果来寻找优化那些复杂的目标。作者将seq2seq的训练过程对应于强化学习抽象成了这么几个部分：\n\n- action: 每一个time step 的候选词\n- state: 每一个time step RNN隐层的神经元状态\n- reward: 整个后续sequence解码完后的bleu等指标\n\n这里需要强调一点，reward是整个sequence解码完的指标，所以作者题目是sequence级别的训练，不是word级别，此文的目标也是优化最后整个生产sequence的效果。\n\n因此作者将Reinforce Loss函数定义为整个生成出的sequence拿到的负reward:\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740434191.png\n)\n对参数\\\\(\\theta\\\\)求导:\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740913152.png\n)\n其中\\\\(O_t\\\\)是softmax的输入。\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740768186.png\n)\n这个公式的推导可以去看另一篇用RL的文章 https://arxiv.org/abs/1505.00521\n需要注意的是 这个导数的右半部分其实就是交叉熵Loss(XENT指在decode部分输入为ground truth的训练方式)的求导结果:\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475741346812.png\n)\n而左半部分是整个被生成序列的reward减去t时刻状态\\\\(S_t\\\\)下所能得到的平均reward，这是强化学习比较常见的一个公式。\n这个推导结果的物理意义可以很直观的解释，即在t+1时刻右端交叉熵Loss的限制下，左端要求我们选择被生成的词最后构成的reward \\\\(r(w_1^g,...,w_T^g)\\\\)要大于在t+1时刻时态的一个可获得的平均reward \\\\({\\bar{r}}_{t+1}\\\\)。\n\n前面提到是sequence级别的训练，所以最终每个时刻的loss要整个sequence全decode后才能得到，而目前我们唯一不知道的量就是reward \\\\({\\bar{r}}_{t+1}\\\\)。在其他场景下，这个值可以用随机的生成结果来采样取得，本文作者用了非常简单的线性回归来预估这个值，输入为当前RNN模型的所有隐层节点状态，Loss为与真值reward二范数距离: \\\\({||\\bar{r}-r||}^2\\\\)。\n\n### 训练过程\n偷个懒直接截原文图再来解释整个过程:\n![](http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475742637703.png)\n\n这里面\\\\(N^{XENT}\\\\)和\\\\(N^{XE+R}\\\\)分别是两个可调的超参，分别代表两个训练过程的训练次数，XENT代表传统的交叉熵Loss训练并且decode时每一个时刻的输入为ground truth \\\\(w_t\\\\), XE+R代表的是Loss为前文提到的Reinforce Loss，T代表的是decode部分sequence长度，decode部分每一个时刻的输入为模型预测生成的输出\\\\(w_t^g\\\\)。\n\n整个过程其实就分为两部分，一开始先训练\\\\(N^{XENT}\\\\)个epoch 用传统的XENT训练方式，之后呢在每一个sequence训练的decode部分，前s个step 用XENT交叉熵loss，后T-s个时态用Reinforce Loss训练。然后不断的将s从T开始减少，最后整个sequence都用Reinforce Loss。\n\n\n这里就不贴实验结果图啦，总的来说在BLEU等整句指标上此文的方法都有显著的提高。\n\n### 我的观点\n- 本文的最大贡献其实不在于Exposure Bias部分而在于Loss-Evaluation Mismatch部分，提出了一个有效的训练方法，能优化那些复杂不直观的目标，同时近来reinforce learning在各领域发力，这种类似的训练方法可以值得大家在各个领域尝试一下。\n\n- 训练过程设计得很巧妙，其实作者很聪明也了解reinforce learning的特性，强化学习有一个致命的缺点就是训练过程的不稳定性，作者让训练过程起始时刻用XENT来作为训练，是为了让模型到达一个较好又稳定的状态，之后再用强化学习来优化模型对于整句的目标。如果不这样安排，大可能模型无法收敛到一个好的解。\n\n- 本文对Exposure Bias还是没有有效解决，这一点上我倒觉得还不如DaD，至少DaD在训练过程的后半程退火阶段，Training的decode过程和Testing的decode过程是一致的，而本文中XE+R部分虽然也是用模型生成的词\\\\(w_t^g\\\\)做输入但是Loss是RL Loss并不是完全依赖于交叉熵。这里是不对等的，所以预测过程用beam search依然会极大的提高本文提出的模型的效果。\n","slug":"RL-for-seq2seq","published":1,"updated":"2016-10-06T08:52:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"city3y3wg0000ujzuf9ekhgjs","content":"<h1 id=\"Sequence级别的seq2seq训练过程，用强化学习来优化整个句子效果\"><a href=\"#Sequence级别的seq2seq训练过程，用强化学习来优化整个句子效果\" class=\"headerlink\" title=\"Sequence级别的seq2seq训练过程，用强化学习来优化整个句子效果\"></a>Sequence级别的seq2seq训练过程，用强化学习来优化整个句子效果</h1><p><a href=\"http://arxiv.org/abs/1511.06732\" target=\"_blank\" rel=\"external\">原论文连接</a></p>\n<p>###Summary</p>\n<p>最近突然想把自己看过的东西记下来，决定开通博客啦，就以这篇seq2seq领域的论文开始吧。</p>\n<p>先简单介绍一下seq2seq是什么。<br>processing of seq2seq：<br><img src=\"https://www.tensorflow.org/versions/r0.11/images/basic_seq2seq.png\" alt=\"\"><br>借用一下tensorflow里Sequence-to-Sequence的图。这个任务的目标简单的说就是将一个序列翻译成另一序列。</p>\n<p>具体到这张图里我们的输入是有序的A,B,C序列，目标序列是输出X,Y,Z。其中每一个白色的方框是RNN的一个Cell，输入部分和输出部分的RNN分别共享两套参数。整个过程可看做将输入的A,B,C通过输入RNN编码为一个Cell code，输出部分根据上一时态的输入以及Cell状态预估下一个输出词的概率分布即<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script> \\(P(w<em>{t+1}^g|h</em>{t+1},w_t)\\) 根据此概率分布找出最适合的输出，可抽象为一个encode之后decode的过程。</p>\n<p>此类过程可以做很多工作，比如机器翻译讲英文翻译为中文，摘要抽取将长文转换为单字信息量更大的短文，或者是人机对话的QA问答都可以抽象成类似的过程。</p>\n<h3 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h3><p>回到这篇论文来看，它主要目标是试图解决seq2seq领域的两个重要问题：</p>\n<ul>\n<li><p>Exposure Bias: seq2seq的Training阶段的decode过程，我们是有ground truth的，即训练decode过程的RNN时我们可以输入真实的序列上一个time step的词，这种训练过程叫XENT。但是在Testing阶段的decode过程我们并不知道上一个time step的词是什么，我们输入给model的是一个预测出来概率最大的词，如果这个词错了，误差会随后续过程传递并且模型没有机会纠正。</p>\n</li>\n<li><p>Loss-Evaluation Mismatch: 我们训练模型的时候，采用的loss是word级别的交叉熵，但是我们最终评判模型的指标是sequence级别的指标，比如机器翻译的BLEU.</p>\n</li>\n</ul>\n<h3 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h3><p>对应于Exposure Bias，该论文之前有两种解决方法：</p>\n<ul>\n<li><p>beam search<br>  算法原理：图路径搜索中，每一步深度扩展的时候，剪掉一些质量比较差的结点，    保留下一些质量较高的结点，这样就减少了空间消耗，并提高了时间效率，但缺点就是    有可能存在潜在的最佳方案被丢弃，其中保留的质量较高路径的个数称为beam size.（走n步吃到嘴多的果实）<br>  <img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475738457157.png\" alt=\"\"><br>应用到seq2seq里就是每一次解码过程看成是一步深度扩展，每一次解码预测概率最大的beam size个词就是候选结点，并累积概率选择概率和最高的beam size条路径，把整个解码过程看成是寻求最大联合概率的图搜索过程。该方法能解决Exposure bias的原因在于它使解码过程不仅仅依赖于前一个词输出，还要满足全局解码概率最大，因而原始模型前一个预测错误而带来的误差传递的可能性就降低了。</p>\n</li>\n<li><p>DaD (Data as Demonstrator)<br>对于Exposure bias造成解码结果不好的原因，bengio解释为由于训练和预测过程在输入数据的分布不同，前者是样本的数据分布，后者则是decode模型的输出分布。因而解决办法就是保证两个流程在解码的时候输入参数服从相同分布，即都采用前一个词的预测结果作为当前词的输入。<br>为此提出了一种退火算法来解决这个问题，在Training过程中引入一个概率值参数，称其为温度，当温度值较大时高概率采用ground truth词\\(w_t\\)输入，当值较小时，则高概率采用预测输出词\\(w_t^g\\)作为下一个输入，随着迭代次数的增加，该参数趋近于0.即完全采用前一个词的预测\\(w_t^g\\)作为输入。<br>Training的decode部分如下图：\\(w_t^g\\)为模型预测词，\\(w_t\\)为ground truth 后文中所有的带上标g的都为模型预测输出。<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475739099129.png\" alt=\"\"></p>\n<h3 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h3><p>此文借鉴了DaD的训练过程，同时由于有些目标如(BLEU)不能被直接优化，作者在此文中提出了一种新的训练流程，能可观的提高此类目标的效果。<br>核心思想是利用强化学习，强化学习可以通过随机的递归生成结果来寻找优化那些复杂的目标。作者将seq2seq的训练过程对应于强化学习抽象成了这么几个部分：</p>\n</li>\n<li><p>action: 每一个time step 的候选词</p>\n</li>\n<li>state: 每一个time step RNN隐层的神经元状态</li>\n<li>reward: 整个后续sequence解码完后的bleu等指标</li>\n</ul>\n<p>这里需要强调一点，reward是整个sequence解码完的指标，所以作者题目是sequence级别的训练，不是word级别，此文的目标也是优化最后整个生产sequence的效果。</p>\n<p>因此作者将Reinforce Loss函数定义为整个生成出的sequence拿到的负reward:<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740434191.png\" alt=\"\"><br>对参数\\(\\theta\\)求导:<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740913152.png\" alt=\"\"><br>其中\\(O_t\\)是softmax的输入。<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740768186.png\" alt=\"\"><br>这个公式的推导可以去看另一篇用RL的文章 <a href=\"https://arxiv.org/abs/1505.00521\" target=\"_blank\" rel=\"external\">https://arxiv.org/abs/1505.00521</a><br>需要注意的是 这个导数的右半部分其实就是交叉熵Loss(XENT指在decode部分输入为ground truth的训练方式)的求导结果:<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475741346812.png\" alt=\"\"><br>而左半部分是整个被生成序列的reward减去t时刻状态\\(S_t\\)下所能得到的平均reward，这是强化学习比较常见的一个公式。<br>这个推导结果的物理意义可以很直观的解释，即在t+1时刻右端交叉熵Loss的限制下，左端要求我们选择被生成的词最后构成的reward \\(r(w_1^g,…,w<em>T^g)\\)要大于在t+1时刻时态的一个可获得的平均reward \\({\\bar{r}}</em>{t+1}\\)。</p>\n<p>前面提到是sequence级别的训练，所以最终每个时刻的loss要整个sequence全decode后才能得到，而目前我们唯一不知道的量就是reward \\({\\bar{r}}_{t+1}\\)。在其他场景下，这个值可以用随机的生成结果来采样取得，本文作者用了非常简单的线性回归来预估这个值，输入为当前RNN模型的所有隐层节点状态，Loss为与真值reward二范数距离: \\({||\\bar{r}-r||}^2\\)。</p>\n<h3 id=\"训练过程\"><a href=\"#训练过程\" class=\"headerlink\" title=\"训练过程\"></a>训练过程</h3><p>偷个懒直接截原文图再来解释整个过程:<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475742637703.png\" alt=\"\"></p>\n<p>这里面\\(N^{XENT}\\)和\\(N^{XE+R}\\)分别是两个可调的超参，分别代表两个训练过程的训练次数，XENT代表传统的交叉熵Loss训练并且decode时每一个时刻的输入为ground truth \\(w_t\\), XE+R代表的是Loss为前文提到的Reinforce Loss，T代表的是decode部分sequence长度，decode部分每一个时刻的输入为模型预测生成的输出\\(w_t^g\\)。</p>\n<p>整个过程其实就分为两部分，一开始先训练\\(N^{XENT}\\)个epoch 用传统的XENT训练方式，之后呢在每一个sequence训练的decode部分，前s个step 用XENT交叉熵loss，后T-s个时态用Reinforce Loss训练。然后不断的将s从T开始减少，最后整个sequence都用Reinforce Loss。</p>\n<p>这里就不贴实验结果图啦，总的来说在BLEU等整句指标上此文的方法都有显著的提高。</p>\n<h3 id=\"我的观点\"><a href=\"#我的观点\" class=\"headerlink\" title=\"我的观点\"></a>我的观点</h3><ul>\n<li><p>本文的最大贡献其实不在于Exposure Bias部分而在于Loss-Evaluation Mismatch部分，提出了一个有效的训练方法，能优化那些复杂不直观的目标，同时近来reinforce learning在各领域发力，这种类似的训练方法可以值得大家在各个领域尝试一下。</p>\n</li>\n<li><p>训练过程设计得很巧妙，其实作者很聪明也了解reinforce learning的特性，强化学习有一个致命的缺点就是训练过程的不稳定性，作者让训练过程起始时刻用XENT来作为训练，是为了让模型到达一个较好又稳定的状态，之后再用强化学习来优化模型对于整句的目标。如果不这样安排，大可能模型无法收敛到一个好的解。</p>\n</li>\n<li><p>本文对Exposure Bias还是没有有效解决，这一点上我倒觉得还不如DaD，至少DaD在训练过程的后半程退火阶段，Training的decode过程和Testing的decode过程是一致的，而本文中XE+R部分虽然也是用模型生成的词\\(w_t^g\\)做输入但是Loss是RL Loss并不是完全依赖于交叉熵。这里是不对等的，所以预测过程用beam search依然会极大的提高本文提出的模型的效果。</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"Sequence级别的seq2seq训练过程，用强化学习来优化整个句子效果\"><a href=\"#Sequence级别的seq2seq训练过程，用强化学习来优化整个句子效果\" class=\"headerlink\" title=\"Sequence级别的seq2seq训练过程，用强化学习来优化整个句子效果\"></a>Sequence级别的seq2seq训练过程，用强化学习来优化整个句子效果</h1><p><a href=\"http://arxiv.org/abs/1511.06732\">原论文连接</a></p>\n<p>###Summary</p>\n<p>最近突然想把自己看过的东西记下来，决定开通博客啦，就以这篇seq2seq领域的论文开始吧。</p>\n<p>先简单介绍一下seq2seq是什么。<br>processing of seq2seq：<br><img src=\"https://www.tensorflow.org/versions/r0.11/images/basic_seq2seq.png\" alt=\"\"><br>借用一下tensorflow里Sequence-to-Sequence的图。这个任务的目标简单的说就是将一个序列翻译成另一序列。</p>\n<p>具体到这张图里我们的输入是有序的A,B,C序列，目标序列是输出X,Y,Z。其中每一个白色的方框是RNN的一个Cell，输入部分和输出部分的RNN分别共享两套参数。整个过程可看做将输入的A,B,C通过输入RNN编码为一个Cell code，输出部分根据上一时态的输入以及Cell状态预估下一个输出词的概率分布即<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script> \\(P(w<em>{t+1}^g|h</em>{t+1},w_t)\\) 根据此概率分布找出最适合的输出，可抽象为一个encode之后decode的过程。</p>\n<p>此类过程可以做很多工作，比如机器翻译讲英文翻译为中文，摘要抽取将长文转换为单字信息量更大的短文，或者是人机对话的QA问答都可以抽象成类似的过程。</p>\n<h3 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h3><p>回到这篇论文来看，它主要目标是试图解决seq2seq领域的两个重要问题：</p>\n<ul>\n<li><p>Exposure Bias: seq2seq的Training阶段的decode过程，我们是有ground truth的，即训练decode过程的RNN时我们可以输入真实的序列上一个time step的词，这种训练过程叫XENT。但是在Testing阶段的decode过程我们并不知道上一个time step的词是什么，我们输入给model的是一个预测出来概率最大的词，如果这个词错了，误差会随后续过程传递并且模型没有机会纠正。</p>\n</li>\n<li><p>Loss-Evaluation Mismatch: 我们训练模型的时候，采用的loss是word级别的交叉熵，但是我们最终评判模型的指标是sequence级别的指标，比如机器翻译的BLEU.</p>\n</li>\n</ul>\n<h3 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h3><p>对应于Exposure Bias，该论文之前有两种解决方法：</p>\n<ul>\n<li><p>beam search<br>  算法原理：图路径搜索中，每一步深度扩展的时候，剪掉一些质量比较差的结点，    保留下一些质量较高的结点，这样就减少了空间消耗，并提高了时间效率，但缺点就是    有可能存在潜在的最佳方案被丢弃，其中保留的质量较高路径的个数称为beam size.（走n步吃到嘴多的果实）<br>  <img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475738457157.png\" alt=\"\"><br>应用到seq2seq里就是每一次解码过程看成是一步深度扩展，每一次解码预测概率最大的beam size个词就是候选结点，并累积概率选择概率和最高的beam size条路径，把整个解码过程看成是寻求最大联合概率的图搜索过程。该方法能解决Exposure bias的原因在于它使解码过程不仅仅依赖于前一个词输出，还要满足全局解码概率最大，因而原始模型前一个预测错误而带来的误差传递的可能性就降低了。</p>\n</li>\n<li><p>DaD (Data as Demonstrator)<br>对于Exposure bias造成解码结果不好的原因，bengio解释为由于训练和预测过程在输入数据的分布不同，前者是样本的数据分布，后者则是decode模型的输出分布。因而解决办法就是保证两个流程在解码的时候输入参数服从相同分布，即都采用前一个词的预测结果作为当前词的输入。<br>为此提出了一种退火算法来解决这个问题，在Training过程中引入一个概率值参数，称其为温度，当温度值较大时高概率采用ground truth词\\(w_t\\)输入，当值较小时，则高概率采用预测输出词\\(w_t^g\\)作为下一个输入，随着迭代次数的增加，该参数趋近于0.即完全采用前一个词的预测\\(w_t^g\\)作为输入。<br>Training的decode部分如下图：\\(w_t^g\\)为模型预测词，\\(w_t\\)为ground truth 后文中所有的带上标g的都为模型预测输出。<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475739099129.png\" alt=\"\"></p>\n<h3 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h3><p>此文借鉴了DaD的训练过程，同时由于有些目标如(BLEU)不能被直接优化，作者在此文中提出了一种新的训练流程，能可观的提高此类目标的效果。<br>核心思想是利用强化学习，强化学习可以通过随机的递归生成结果来寻找优化那些复杂的目标。作者将seq2seq的训练过程对应于强化学习抽象成了这么几个部分：</p>\n</li>\n<li><p>action: 每一个time step 的候选词</p>\n</li>\n<li>state: 每一个time step RNN隐层的神经元状态</li>\n<li>reward: 整个后续sequence解码完后的bleu等指标</li>\n</ul>\n<p>这里需要强调一点，reward是整个sequence解码完的指标，所以作者题目是sequence级别的训练，不是word级别，此文的目标也是优化最后整个生产sequence的效果。</p>\n<p>因此作者将Reinforce Loss函数定义为整个生成出的sequence拿到的负reward:<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740434191.png\" alt=\"\"><br>对参数\\(\\theta\\)求导:<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740913152.png\" alt=\"\"><br>其中\\(O_t\\)是softmax的输入。<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475740768186.png\" alt=\"\"><br>这个公式的推导可以去看另一篇用RL的文章 <a href=\"https://arxiv.org/abs/1505.00521\">https://arxiv.org/abs/1505.00521</a><br>需要注意的是 这个导数的右半部分其实就是交叉熵Loss(XENT指在decode部分输入为ground truth的训练方式)的求导结果:<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475741346812.png\" alt=\"\"><br>而左半部分是整个被生成序列的reward减去t时刻状态\\(S_t\\)下所能得到的平均reward，这是强化学习比较常见的一个公式。<br>这个推导结果的物理意义可以很直观的解释，即在t+1时刻右端交叉熵Loss的限制下，左端要求我们选择被生成的词最后构成的reward \\(r(w_1^g,…,w<em>T^g)\\)要大于在t+1时刻时态的一个可获得的平均reward \\({\\bar{r}}</em>{t+1}\\)。</p>\n<p>前面提到是sequence级别的训练，所以最终每个时刻的loss要整个sequence全decode后才能得到，而目前我们唯一不知道的量就是reward \\({\\bar{r}}_{t+1}\\)。在其他场景下，这个值可以用随机的生成结果来采样取得，本文作者用了非常简单的线性回归来预估这个值，输入为当前RNN模型的所有隐层节点状态，Loss为与真值reward二范数距离: \\({||\\bar{r}-r||}^2\\)。</p>\n<h3 id=\"训练过程\"><a href=\"#训练过程\" class=\"headerlink\" title=\"训练过程\"></a>训练过程</h3><p>偷个懒直接截原文图再来解释整个过程:<br><img src=\"http://7xp3xc.com1.z0.glb.clouddn.com/201601/1475742637703.png\" alt=\"\"></p>\n<p>这里面\\(N^{XENT}\\)和\\(N^{XE+R}\\)分别是两个可调的超参，分别代表两个训练过程的训练次数，XENT代表传统的交叉熵Loss训练并且decode时每一个时刻的输入为ground truth \\(w_t\\), XE+R代表的是Loss为前文提到的Reinforce Loss，T代表的是decode部分sequence长度，decode部分每一个时刻的输入为模型预测生成的输出\\(w_t^g\\)。</p>\n<p>整个过程其实就分为两部分，一开始先训练\\(N^{XENT}\\)个epoch 用传统的XENT训练方式，之后呢在每一个sequence训练的decode部分，前s个step 用XENT交叉熵loss，后T-s个时态用Reinforce Loss训练。然后不断的将s从T开始减少，最后整个sequence都用Reinforce Loss。</p>\n<p>这里就不贴实验结果图啦，总的来说在BLEU等整句指标上此文的方法都有显著的提高。</p>\n<h3 id=\"我的观点\"><a href=\"#我的观点\" class=\"headerlink\" title=\"我的观点\"></a>我的观点</h3><ul>\n<li><p>本文的最大贡献其实不在于Exposure Bias部分而在于Loss-Evaluation Mismatch部分，提出了一个有效的训练方法，能优化那些复杂不直观的目标，同时近来reinforce learning在各领域发力，这种类似的训练方法可以值得大家在各个领域尝试一下。</p>\n</li>\n<li><p>训练过程设计得很巧妙，其实作者很聪明也了解reinforce learning的特性，强化学习有一个致命的缺点就是训练过程的不稳定性，作者让训练过程起始时刻用XENT来作为训练，是为了让模型到达一个较好又稳定的状态，之后再用强化学习来优化模型对于整句的目标。如果不这样安排，大可能模型无法收敛到一个好的解。</p>\n</li>\n<li><p>本文对Exposure Bias还是没有有效解决，这一点上我倒觉得还不如DaD，至少DaD在训练过程的后半程退火阶段，Training的decode过程和Testing的decode过程是一致的，而本文中XE+R部分虽然也是用模型生成的词\\(w_t^g\\)做输入但是Loss是RL Loss并不是完全依赖于交叉熵。这里是不对等的，所以预测过程用beam search依然会极大的提高本文提出的模型的效果。</p>\n</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}